# MeanFlows
We investigate one-step generative modeling with Mean Flows under data-constrained regimes and extend the framework to structured molecular latent spaces. Mean Flows learn average velocity fields between time pairs, enabling single-step (1-NFE) sampling. While prior work demonstrated strong performance at large scale, its behavior under limited data and reduced training budgets remains unexplored. We conduct systematic ablation studies on a 5,000-image ImageNet subset, analyzing time-pair sampling, Jacobianâ€“vector product (JVP) formulation, positional embeddings, adaptive loss weighting, and classifier-free guidance. We observe scale-dependent deviations from previously reported optima, revealing that certain design choices (e.g., full $r \neq t$ sampling) become preferable in low-data regimes.

Beyond images, we adapt Mean Flows to the 56-dimensional latent space of a Junction Tree VAE for molecular generation. PCA-based distribution analysis shows that the learned flow reshapes Gaussian priors toward the encoded molecular manifold. One-step sampling yields chemically valid molecules (99.6\% validity), demonstrating that Mean Flows can operate on structured scientific latent spaces. Our results suggest that one-step generative flows remain stable under constrained training and can generalize beyond image domains.
